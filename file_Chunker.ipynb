{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AriZu\\.conda\\envs\\pytorchstudy\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  \n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 手动设置 Tesseract 可执行文件路径\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"D:\\Program Files (x86)\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣  加载 NLP 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 预训练 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣  设置 PDF 目录\n",
    "pdf_folder = \"Raw_file_folder\"\n",
    "output_folder = \"Chunk_file_folder\"  # 存储处理后的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\" 使用 PyMuPDF (fitz) 提取 PDF 纯文本 \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text(\"text\"))\n",
    "    return \"\\n\".join(filter(None, text))\n",
    "\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    \"\"\" 提取 PDF 中的表格 \"\"\"\n",
    "    import pdfplumber\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables.extend(page.extract_tables())\n",
    "    return tables\n",
    "\n",
    "\n",
    "def extract_text_from_images(pdf_path):\n",
    "    \"\"\" 使用 PyMuPDF (fitz) 提取 PDF 中的图片并进行 OCR 解析 \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_texts = []\n",
    "    \n",
    "    for page in doc:\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            extracted_texts.append(text.strip())\n",
    "\n",
    "    return \"\\n\".join(extracted_texts)\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\" 使用 spaCy 进行句子分割 \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def segment_paragraphs(text, similarity_threshold=0.6, min_sentences=5):\n",
    "    \"\"\" 使用 BERT 计算语义相似度，智能划分段落 \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    embeddings = bert_model.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "    similarities = [\n",
    "        cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "        for i in range(len(sentences) - 1)\n",
    "    ]\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = [sentences[0]]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        if similarities[i - 1] < similarity_threshold or len(current_paragraph) >= min_sentences:\n",
    "            paragraphs.append(\" \".join(current_paragraph))\n",
    "            current_paragraph = [sentences[i]]\n",
    "        else:\n",
    "            current_paragraph.append(sentences[i])\n",
    "\n",
    "    paragraphs.append(\" \".join(current_paragraph))\n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing：Complex PTSD_ From Surviving to Thriving.pdf\n",
      "Processing done! 文本存储在 Chunk_file_folder\\Complex PTSD_ From Surviving to Thriving_text.txt，表格存储在 Chunk_file_folder\\Complex PTSD_ From Surviving to Thriving_tables.txt\n",
      "\n",
      "processing：GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs.pdf\n",
      "Processing done! 文本存储在 Chunk_file_folder\\GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs_text.txt，表格存储在 Chunk_file_folder\\GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs_tables.txt\n",
      "\n",
      "processing：therapists_guide_to_brief_cbtmanual.pdf\n",
      "Processing done! 文本存储在 Chunk_file_folder\\therapists_guide_to_brief_cbtmanual_text.txt，表格存储在 Chunk_file_folder\\therapists_guide_to_brief_cbtmanual_tables.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 处理 Raw_file_folder 目录中的所有 PDF 文件\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    print(f\"processing：{pdf_file}\")\n",
    "\n",
    "    # 解析 PDF 文本、表格、OCR\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    ocr_text = extract_text_from_images(pdf_path)\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "    # 合并文本进行段落智能划分\n",
    "    full_text = pdf_text + \"\\n\" + ocr_text\n",
    "    segmented_paragraphs = segment_paragraphs(full_text, similarity_threshold=0.6, min_sentences=5)\n",
    "\n",
    "    # 输出结果存储\n",
    "    output_text_path = os.path.join(output_folder, pdf_file.replace(\".pdf\", \"_text.txt\"))\n",
    "    output_table_path = os.path.join(output_folder, pdf_file.replace(\".pdf\", \"_tables.txt\"))\n",
    "\n",
    "    # 存储文本\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, para in enumerate(segmented_paragraphs, 1):\n",
    "            f.write(f\"chunk {i} (Number of characters: {len(para)}):\\n{para}\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    with open(output_table_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for table in tables:\n",
    "            for row in table:\n",
    "                f.write(\"\\t\".join(map(str, row)) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Processing done! 文本存储在 {output_text_path}，表格存储在 {output_table_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
