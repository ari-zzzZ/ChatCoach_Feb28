{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AriZu\\.conda\\envs\\pytorchstudy\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  \n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 手动设置 Tesseract 可执行文件路径\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"D:\\Program Files (x86)\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 语义切割模型\n",
    "'''nlp = spacy.load(\"en_core_web_sm\") #用于句子分割，将长文本拆成句子\n",
    "\n",
    "if not nlp.has_pipe(\"sentencizer\"):\n",
    "    nlp.add_pipe(\"sentencizer\")'''\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # 只加载分句功能\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")  # 轻量bert，计算文本相似度，并分段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置输入与输出文档\n",
    "pdf_folder = \"Raw_file_folder\"\n",
    "output_folder = \"Chunk_file_folder\"  # 存储处理后的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\" 使用 PyMuPDF (fitz) 提取 PDF 纯文本 \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text(\"text\"))\n",
    "    return \"\\n\".join(filter(None, text))\n",
    "\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    \"\"\" 提取 PDF 中的表格 \"\"\"\n",
    "    import pdfplumber\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables.extend(page.extract_tables())\n",
    "    return tables\n",
    "\n",
    "\n",
    "def extract_text_from_images(pdf_path):\n",
    "    \"\"\" 使用 PyMuPDF (fitz) 提取 PDF 中的图片并进行 OCR 解析 \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_texts = []\n",
    "    \n",
    "    for page in doc:\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            extracted_texts.append(text.strip())\n",
    "\n",
    "    return \"\\n\".join(extracted_texts)\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\" 使用 spaCy 进行句子分割 \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "'''此函数\n",
    "输入：一段长文本 text\n",
    "输出：一个列表 paragraphs，其中每个元素都是一个智能划分后的段落。'''\n",
    "def segment_paragraphs(text, similarity_threshold, min_sentences):\n",
    "    \"\"\"使用 BERT 计算语义相似度，智能划分段落\"\"\"\n",
    "    \n",
    "    # 句子分割\n",
    "    sentences = split_into_sentences(text)\n",
    "    if not sentences:  # 处理空输入\n",
    "        return []\n",
    "\n",
    "    # embedding 把句子转换为向量\n",
    "    embeddings = bert_model.encode(sentences, convert_to_numpy=True)\n",
    "    \n",
    "    # 计算滑动窗口相似度（每 3 句话计算一次相似度）\n",
    "    '''遍历 sentences，对于每个句子 i，计算：\n",
    "sentence[i] 和 sentence[i+1] 之间的余弦相似度。\n",
    "sentence[i+1] 和 sentence[i+2] 之间的余弦相似度。\n",
    "取这两个相似度的平均值，作为 sentence[i] 的相似度评分'''\n",
    "    similarities = []\n",
    "    for i in range(len(sentences) - 2):\n",
    "        avg_similarity = np.mean([\n",
    "            cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0],\n",
    "            cosine_similarity([embeddings[i + 1]], [embeddings[i + 2]])[0][0]\n",
    "        ])\n",
    "        similarities.append(avg_similarity)\n",
    "\n",
    "    # 处理最后两个句子的相似度，避免遗漏\n",
    "    if len(sentences) > 2:\n",
    "        similarities.append(cosine_similarity([embeddings[-2]], [embeddings[-1]])[0][0])\n",
    "\n",
    "    # 按相似度划分段落\n",
    "    paragraphs = [] #存储最终的段落结果\n",
    "    current_paragraph = [sentences[0]] #当前正在构建的段落，初始时包含第一句话\n",
    "\n",
    "    # 用滑动窗口的平均相似度来决定是否拆分 chunk\n",
    "    for i in range(1, len(sentences)):\n",
    "        #如果相似度低, 或当前段落已经有 min_sentences， 则拆分\n",
    "        if i - 1 < len(similarities) and (similarities[i - 1] < similarity_threshold or len(current_paragraph) >= min_sentences):\n",
    "            paragraphs.append(\" \".join(current_paragraph))\n",
    "            current_paragraph = [sentences[i]]\n",
    "        else: #否则，当前句子 sentences[i] 继续加入 current_paragraph\n",
    "            current_paragraph.append(sentences[i])\n",
    "\n",
    "    # 添加最后一个段落\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing：Complex PTSD_ From Surviving to Thriving.pdf\n",
      "Processing done! Text stored in Chunk_file_folder\\Complex PTSD_ From Surviving to Thriving_text.txt，Tables stored in Chunk_file_folder\\Complex PTSD_ From Surviving to Thriving_tables.txt\n",
      "\n",
      "processing：GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs.pdf\n",
      "Processing done! Text stored in Chunk_file_folder\\GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs_text.txt，Tables stored in Chunk_file_folder\\GPMHSC-Suicide-prevention-and-first-aid-resource-for-GPs_tables.txt\n",
      "\n",
      "processing：therapists_guide_to_brief_cbtmanual.pdf\n",
      "Processing done! Text stored in Chunk_file_folder\\therapists_guide_to_brief_cbtmanual_text.txt，Tables stored in Chunk_file_folder\\therapists_guide_to_brief_cbtmanual_tables.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 处理 Raw_file_folder 目录中的所有 PDF 文件\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    print(f\"processing：{pdf_file}\")\n",
    "\n",
    "    # 解析 PDF 文本、表格、OCR\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    ocr_text = extract_text_from_images(pdf_path)\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "    # 合并文本进行段落智能划分\n",
    "    full_text = pdf_text + \"\\n\" + ocr_text\n",
    "    segmented_paragraphs = segment_paragraphs(full_text, similarity_threshold=0.15, min_sentences=8)\n",
    "\n",
    "    # 输出结果存储\n",
    "    output_text_path = os.path.join(output_folder, pdf_file.replace(\".pdf\", \"_text.txt\"))\n",
    "    output_table_path = os.path.join(output_folder, pdf_file.replace(\".pdf\", \"_tables.txt\"))\n",
    "\n",
    "    # 存储文本\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, para in enumerate(segmented_paragraphs, 1):\n",
    "            f.write(f\"chunk {i} (Number of characters: {len(para)}):\\n{para}\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    with open(output_table_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for table in tables:\n",
    "            for row in table:\n",
    "                f.write(\"\\t\".join(map(str, row)) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Processing done! Text stored in {output_text_path}，Tables stored in {output_table_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
