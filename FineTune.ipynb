{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AriZu\\.conda\\envs\\TRLfinetune\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model_path = \"D:\\HuggingFace\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct\\snapshots\\989aa7980e4cf806f80c7fef2b1adb7bc71aa306\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"nbertagnolli/counsel-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][2774]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to OAI messages\n",
    " \n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a mental therapist.\"},\n",
    "      {\"role\": \"user\", \"content\": sample[\"questionText\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answerText\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(create_conversation, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a mental therapist.', 'role': 'system'}, {'content': 'What are some difficulties that a counselor can encounter when dealing with a client?', 'role': 'user'}, {'content': 'Each counselor will have their own list of \"difficulties\" in doing therapy work with a client. \\xa0Even if clinically trained similarly, since counselors are human then their response to your question will reflect their unique differences as humans.On my list is when the emotional pain I feel for someone describing some type of injustice or unfair treatment by another, feels very deep.Sometimes I feel like avoiding the pain I feel by asking questions which will steer the conversation away from the painful areas the client talks about.What in fact is necessary to clear out their pain, is to step further into so as to realize their emotional pain isn\\'t greater than who they are.', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][2774])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a mental therapist.', 'role': 'system'}, {'content': \"I'm a teenager. My dad has been jail for the last five years. It's tough, but my mom really tries to give a normal life to my two sisters, my brother, and I. I feel like I took upon a parent role when I'm the second youngest, and I'm not stable. My mother and sisters say I'm overdramatic. I’m just so hurt, and I keep breaking down.\", 'role': 'user'}, {'content': \"It sounds like you have a lot of weight on your shoulders.I'm not sure what you mean when you say you're not stable and you are breaking down. If you are crying because you're sad, that is okay. If you are crying a lot or having trouble eating or sleeping, that's different than just crying sometimes because you are sad. It may be helpful to talk with a local mental health professional in your area. They can help you to figure out what you could do differently to have the role in your family that you would like while also supporting yourself and what you want.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    " \n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # 适配 Qwen 结构\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=1,                       # log every 10 steps\n",
    "    save_strategy=\"no\",                  # save checkpoint every epoch\n",
    "    \n",
    "    save_steps=1,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    learning_rate=3e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset size: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_llama(example):\n",
    "    messages = example[\"messages\"]\n",
    "    formatted_text = \"<s>\"  # 开始标记\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"system\":\n",
    "            formatted_text += f\"[INST]<<SYS>>{content}<</SYS>>\\n\"\n",
    "        elif role == \"user\":\n",
    "            formatted_text += f\"{content} [/INST]\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted_text += f\"{content} </s>\"\n",
    "    return {\"text\": formatted_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2775/2775 [00:00<00:00, 15910.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(format_to_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]<<SYS>>You are a mental therapist.<</SYS>>\n",
      "What are some difficulties that a counselor can encounter when dealing with a client? [/INST]Each counselor will have their own list of \"difficulties\" in doing therapy work with a client.  Even if clinically trained similarly, since counselors are human then their response to your question will reflect their unique differences as humans.On my list is when the emotional pain I feel for someone describing some type of injustice or unfair treatment by another, feels very deep.Sometimes I feel like avoiding the pain I feel by asking questions which will steer the conversation away from the painful areas the client talks about.What in fact is necessary to clear out their pain, is to step further into so as to realize their emotional pain isn't greater than who they are. </s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][2774][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    " \n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",  # 指定训练字段\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "print(f\"Trainable ratio: {trainable_params / total_params:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 338/378 [3:11:53<23:31, 35.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2343, 'learning_rate': 0.0003, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 339/378 [3:12:28<22:52, 35.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3407, 'learning_rate': 0.0003, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 340/378 [3:13:02<22:07, 34.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0989, 'learning_rate': 0.0003, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 341/378 [3:13:36<21:25, 34.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0259, 'learning_rate': 0.0003, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 342/378 [3:14:11<20:46, 34.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2048, 'learning_rate': 0.0003, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 343/378 [3:14:45<20:09, 34.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1544, 'learning_rate': 0.0003, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 344/378 [3:15:19<19:32, 34.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2, 'learning_rate': 0.0003, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 345/378 [3:15:54<18:56, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1658, 'learning_rate': 0.0003, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 346/378 [3:16:28<18:21, 34.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9663, 'learning_rate': 0.0003, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 347/378 [3:17:03<17:46, 34.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3009, 'learning_rate': 0.0003, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 348/378 [3:17:37<17:11, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0949, 'learning_rate': 0.0003, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 349/378 [3:18:11<16:34, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0864, 'learning_rate': 0.0003, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 350/378 [3:18:45<15:58, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2548, 'learning_rate': 0.0003, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 351/378 [3:19:19<15:22, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2219, 'learning_rate': 0.0003, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 352/378 [3:19:53<14:47, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3564, 'learning_rate': 0.0003, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 353/378 [3:20:27<14:12, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2383, 'learning_rate': 0.0003, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 354/378 [3:21:01<13:38, 34.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2226, 'learning_rate': 0.0003, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 355/378 [3:21:35<13:03, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5502, 'learning_rate': 0.0003, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 356/378 [3:22:09<12:29, 34.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1369, 'learning_rate': 0.0003, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 357/378 [3:22:43<11:55, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1887, 'learning_rate': 0.0003, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 358/378 [3:23:17<11:21, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3483, 'learning_rate': 0.0003, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 359/378 [3:23:51<10:46, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2353, 'learning_rate': 0.0003, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 360/378 [3:24:25<10:12, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1983, 'learning_rate': 0.0003, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 361/378 [3:24:59<09:38, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1749, 'learning_rate': 0.0003, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 362/378 [3:25:34<09:04, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1676, 'learning_rate': 0.0003, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 363/378 [3:26:08<08:30, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0476, 'learning_rate': 0.0003, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 364/378 [3:26:42<07:56, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.171, 'learning_rate': 0.0003, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 365/378 [3:27:16<07:22, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1229, 'learning_rate': 0.0003, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 366/378 [3:27:50<06:48, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2351, 'learning_rate': 0.0003, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 367/378 [3:28:24<06:14, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2292, 'learning_rate': 0.0003, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 368/378 [3:28:58<05:40, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'learning_rate': 0.0003, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 369/378 [3:29:32<05:06, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4216, 'learning_rate': 0.0003, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 370/378 [3:30:06<04:32, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2382, 'learning_rate': 0.0003, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 371/378 [3:30:40<03:58, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3067, 'learning_rate': 0.0003, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 372/378 [3:31:14<03:24, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3141, 'learning_rate': 0.0003, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 373/378 [3:31:48<02:50, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4065, 'learning_rate': 0.0003, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 374/378 [3:32:22<02:16, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0921, 'learning_rate': 0.0003, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 375/378 [3:32:56<01:42, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1479, 'learning_rate': 0.0003, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 376/378 [3:33:30<01:08, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2905, 'learning_rate': 0.0003, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 377/378 [3:34:04<00:34, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2817, 'learning_rate': 0.0003, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [3:34:38<00:00, 34.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3414, 'learning_rate': 0.0003, 'epoch': 3.0}\n",
      "{'train_runtime': 12878.7358, 'train_samples_per_second': 0.059, 'train_steps_per_second': 0.029, 'train_loss': 1.6323963034720648, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=378, training_loss=1.6323963034720648, metrics={'train_runtime': 12878.7358, 'train_samples_per_second': 0.059, 'train_steps_per_second': 0.029, 'train_loss': 1.6323963034720648, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model(\"saved_model\")  # 保存当前模型到 \"saved_model\" 文件夹\n",
    "tokenizer.save_pretrained(\"saved_model\")  # 保存 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def format_data(sample):\n",
    "    # 获取 messages 并解包\n",
    "    messages = sample.get(\"messages\", [])\n",
    "\n",
    "    # 确保 messages 是 list 且非空\n",
    "    if not isinstance(messages, list) or not messages:\n",
    "        print(f\"Skipping due to invalid messages format: {sample}\")\n",
    "        return None\n",
    "\n",
    "    # 如果 messages 是双重 list，解包它\n",
    "    if isinstance(messages[0], list):\n",
    "        messages = messages[0]  # 取出内部 list 作为最终的 messages\n",
    "\n",
    "    # 遍历 messages，确保所有字段不是 None\n",
    "    for msg in messages:\n",
    "        if msg[\"content\"] is None:\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                msg[\"content\"] = \"I'm here to help. Could you tell me more about what you're experiencing?\"\n",
    "            else:\n",
    "                msg[\"content\"] = \"No input provided.\"\n",
    "\n",
    "    try:\n",
    "        # 应用 tokenizer\n",
    "        formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        tokenized = tokenizer(formatted_text, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "        return {\"input_ids\": tokenized}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {messages}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# 重新预处理数据\n",
    "train_dataset = dataset[\"train\"].map(format_data)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRLfinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
